# -*- coding: utf-8 -*-
"""Autonomy - 20230517 - Warrington Report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VZnSPwBm1OgLGL78BxOXv5kP2JZ5jrCp
"""

#@title
#onet analysis
import os
import pandas as pd
from zipfile import ZipFile
import requests

url = 'https://www.onetcenter.org/dl_files/database/db_26_0_text.zip'
filename = 'onet_db.zip'

# Download and extract the O*NET database
if not os.path.exists(filename):
    response = requests.get(url)
    with open(filename, 'wb') as f:
        f.write(response.content)

onet_db = ZipFile('onet_db.zip')
onet_data={text_file.filename: pd.read_csv(onet_db.open(text_file.filename), sep='\t', header=0)
       for text_file in onet_db.infolist()
       if text_file.filename.endswith('.txt')}

onet_data = { k.replace('db_26_0_text/', ''): v for k, v in onet_data.items() }
onet_data = { k.replace('.txt', ''): v for k, v in onet_data.items() }
del onet_data['Read Me']
onet_keys=onet_data.keys()
iobA=['Abilities', 'Work Activities', 'Skills', 'Knowledge']

mapping=pd.read_csv('/content/drive/MyDrive/Autonomy Data Unit v2/Projects/warrington_project/ASPECTT/ASPECTT_Mapping_V0.csv',dtype={'ONS SOC 2010': str,'ISCO08': str,'ISCO-08 Code': str})
mapping['SOC 4-digit code']=mapping['ONS SOC 2010'].str[:4]
newmap=mapping[['O*NET-SOC Code','SOC 4-digit code']]
newmap

def get_pivot_df(onet_data, i, newmap):
    df = onet_data[i]
    df = df[df['Scale ID'] == 'LV']
    pivot_df = pd.pivot_table(
        df.merge(newmap).groupby(['SOC 4-digit code', 'Element Name']).mean()['Data Value'].reset_index(),
        index='SOC 4-digit code',
        columns='Element Name',
        values='Data Value'
    )
    return pivot_df.T

pivot_scores=[]
iobA=['Abilities', 'Work Activities', 'Skills', 'Knowledge']
for i in iobA:
  pivot_scores.append(get_pivot_df(onet_data, i, newmap))
pvt=pivot_scores[0].T
pvt['mapcluster']=pvt.index.astype('int').map(adfmap['cl'].to_dict())
pvt.groupby(['mapcluster']).mean().T

"""#Part 1 - Skills - Confirm with Will if this is only text?

2013 to 2020 Luiz/ aspectt analysis
Small conclusions


Here insert: 

David Brennan analysis
Metrodynamics

#Part 2 - Skills in Warrington
Demand
Adzuna data

##NORTHWEST
"""

!pip install py7zr
import py7zr

# Path to the 7zip file
zip_file = '/content/drive/MyDrive/AUTONOMY/nw.7z'

# Directory path where you want to extract the contents
extract_directory = '/content/NW'

# Open the 7zip file
with py7zr.SevenZipFile(zip_file, mode='r') as archive:
    # Extract all contents to the specified directory
    archive.extractall(path=extract_directory)

"""###CHECK DATASET TIME SERIES"""

x = df
ss=[]
# Loop through every column in the dataset
for col in x.columns:
    if x[col].dtype == 'O':  # Only perform the operation on object (string) columns
        # Count the number of responses in each category
        grouped = x[col].value_counts().reset_index(name='count')

        # Sort the data by count in descending order
       
        sorted_data = grouped.sort_values('count', ascending=False)
        sorted_data=sorted_data[sorted_data['index']!='-']
        print(col, sorted_data.head(3))

import os
import pandas as pd
import re

# Directory path where the CSV files are located
csv_directory = '/content/NW/content'

# List to store individual DataFrames from each CSV file
dataframes = []

# Iterate over the files in the directory
for filename in os.listdir(csv_directory):
    if filename.endswith('.csv'):
        # Read the CSV file and select specific columns
        filepath = os.path.join(csv_directory, filename)
        df = pd.read_csv(filepath, usecols=['filename','LAD22CD', 'salary_predicted', 'contract_time', 'soc2020', 'soc2020_minor_group', 'id', 'category_name','soc2020_major_group'])
        dataframes.append(df)

# Concatenate the DataFrames into a single DataFrame
combined_df = pd.concat(dataframes, ignore_index=True)

# now add colum with extracted month numbers

strings = combined_df['filename']

# Regular expression pattern to match the month number
pattern = r'month=(\d{1,2})'

# List to store extracted month numbers
months = []

# Loop through the strings and extract the month number
for string in strings:
    match = re.search(pattern, string)
    if match:
        month = match.group(1)
        months.append(month)

# add colum with extracted month numbers
combined_df['m']=months

k=combined_df.copy()

import pandas as pd
def create_pivot_table(df, values_col, index_col, column_col):
    pivot_table = pd.pivot_table(df, values=values_col, index=[index_col], columns=[column_col], aggfunc='sum')
    return pivot_table
#fulltime tables
_time='full_time'
c=[]
for i in k.m.unique():
  df2=k.copy()
  df2=df2[df2['contract_time']==_time]
  df2=df2[df2['m']==i]
  pdf=df2[['LAD22CD','soc2020_major_group']].groupby(['LAD22CD','soc2020_major_group']).size().reset_index()
  c.append(create_pivot_table(pdf, 0, 'LAD22CD', 'soc2020_major_group'))

import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis
    pivot_table.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))
    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')
# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')
# adjust layout to prevent overlap of axis labels
fig.tight_layout()


import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis

    # concatenate the top 5 and all the rest into a new pivot table
    # get the sum of the top 5 wards
    top_10_sum = pivot_table.sum(axis=1)
    top10=pivot_table.loc[top_10_sum.sort_values().iloc[-10:].index]
    top10.loc['All the Rest']=pivot_table.loc[top_10_sum.sort_values().iloc[:-10].index].sum()
    pivot_table_concat=top10
    pivot_table_concat.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))

    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')

# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
all_the_rest_label = 'All the Rest'
labels[-1] = all_the_rest_label
handles.append(plt.Rectangle((0,0),1,1,fc='gray', edgecolor = 'none'))
labels.append(all_the_rest_label)

fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')

# adjust layout to prevent overlap of axis labels
fig.tight_layout()

import pandas as pd
def create_pivot_table(df, values_col, index_col, column_col):
    pivot_table = pd.pivot_table(df, values=values_col, index=[index_col], columns=[column_col], aggfunc='sum')
    return pivot_table
#parttime tables
_time='part_time'
c=[]
for i in k.m.unique():
  df2=k.copy()
  df2=df2[df2['contract_time']==_time]
  df2=df2[df2['m']==i]
  pdf=df2[['LAD22CD','soc2020_major_group']].groupby(['LAD22CD','soc2020_major_group']).size().reset_index()
  c.append(create_pivot_table(pdf, 0, 'LAD22CD', 'soc2020_major_group'))

import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis
    pivot_table.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))
    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')
# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')
# adjust layout to prevent overlap of axis labels
fig.tight_layout()


import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis

    # concatenate the top 5 and all the rest into a new pivot table
    # get the sum of the top 5 wards
    top_10_sum = pivot_table.sum(axis=1)
    top10=pivot_table.loc[top_10_sum.sort_values().iloc[-10:].index]
    top10.loc['All the Rest']=pivot_table.loc[top_10_sum.sort_values().iloc[:-10].index].sum()
    pivot_table_concat=top10
    pivot_table_concat.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))

    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')

# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
handles.append(plt.Rectangle((0,0),1,1,fc='gray', edgecolor = 'none'))
fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')

# adjust layout to prevent overlap of axis labels
fig.tight_layout()

c=[]
for i in k.m.unique():
  df2=k.copy()
  df2=df2[df2['contract_time']==_time]
  df2=df2[df2['m']==i]
  pdf=df2[['LAD22CD','category_name']].groupby(['LAD22CD','category_name']).size().reset_index()
  c.append(create_pivot_table(pdf, 0, 'LAD22CD', 'category_name'))


import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis
    pivot_table.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))
    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')
# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')
# adjust layout to prevent overlap of axis labels
fig.tight_layout()


import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis

    # concatenate the top 5 and all the rest into a new pivot table
    # get the sum of the top 5 wards
    top_10_sum = pivot_table.sum(axis=1)
    top10=pivot_table.loc[top_10_sum.sort_values().iloc[-10:].index]
    top10.loc['All the Rest']=pivot_table.loc[top_10_sum.sort_values().iloc[:-10].index].sum()
    pivot_table_concat=top10
    pivot_table_concat.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))

    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')

# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
handles.append(plt.Rectangle((0,0),1,1,fc='gray', edgecolor = 'none'))
fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')

# adjust layout to prevent overlap of axis labels
fig.tight_layout()

"""##Azuna Baseline different regional levels"""

#@title
!wget 'https://files.planning.data.gov.uk/dataset/local-authority-district.geojson'
!pip install geopandas
import pandas as pd
!pip install geopandas
!pip install contextily
!pip install descartes
!pip install shapely
!pip install mapclassify
import geopandas as gpd
import matplotlib.pyplot as plt
import descartes
import shapely
import seaborn as sns
from shapely.geometry import Point, Polygon
import mapclassify
import seaborn as sns
import matplotlib.cm as cm
import geopandas as gpd
# Read the GeoJSON file using GeoPandas
gdf = gpd.read_file('/content/local-authority-district.geojson')
# Print the GeoDataFrame
print(gdf)

"""###united kingdom

##need to generate a regional agg for uk

###northwest
"""

#@title
#select one dataset

#@title
k=combined_df.copy()

#@title
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import geopandas as gpd

x='/content/Local_Authority_Districts_(December_2022)_Names_and_Codes_in_the_United_Kingdom.csv'
xs=pd.read_csv(x)[['LAD22CD','LAD22NM']]
xs.index=xs['LAD22CD']
xs['LAD22NM'].to_dict()

lad_codes= ['E08000003', 'E08000006', 'E07000118', 'E08000009', 'E08000001',
       'E08000002', 'E07000117', 'E08000012', 'E08000008', 'E06000050',
       'E06000049', 'E08000007', 'E07000127', 'E07000028', 'E06000007',
       'E07000125', 'E07000031', 'E08000013', 'E07000121', 'E07000122',
       'E06000008', 'E08000014', 'E08000015', 'E08000010', 'E07000027',
       'E07000026', 'E06000006', 'E08000004', 'E08000011', 'E07000030',
       'E07000126', 'E08000005', 'E07000124', 'E07000029', 'E07000119',
       'E07000123', 'E07000128', 'E07000120']

def visualize_data(_time):
    # Step 1
    df2 = k.copy()
    df2 = df2[df2['contract_time'] == _time]
    kdf = df2.drop_duplicates('id')

    # Step 2
    sonia4clusters = pd.read_csv('/content/drive/MyDrive/AUTONOMY/20230512AspectClustering.csv')
    d = sonia4clusters[['SOC', 'soniasexpandedfinalfix']]
    d.index = d.SOC
    d_dict = d['soniasexpandedfinalfix'].to_dict()

    # Step 3
    kdf['cluster3'] = kdf['soc2020_minor_group'].map(d_dict)
    kdf['cluster4'] = kdf['soc2020'].map(d_dict)
    new = kdf.loc[kdf['cluster3'].dropna().index]
    new['cluster3and4'] = np.where(pd.isna(new['cluster4']), new['cluster3'], new['cluster4'])

    # Step 5
    kdf['soc2020'].dropna().hist()
    plt.show()

    # Step 6
    kdf['soc2020_minor_group'].dropna().hist()
    plt.show()

    # Step 5
    news = new.groupby(['cluster3and4', 'm']).size()
    pivot_table = pd.pivot_table(news.reset_index(), columns='cluster3and4', index='m', values=0)
    pivot_table=pivot_table.sort_index()
    pivot_table.plot(kind='bar', stacked=True)

    # Step 5
    pv = pd.pivot_table(new, columns='cluster3and4', index='m', values='salary_predicted', aggfunc='mean')
    pv.plot()

    # Plot 1 - Pie Chart for Payroll Demand
    annualpayrollequivalent_demand = new[['salary_predicted', 'cluster3and4']].groupby(['cluster3and4']).sum()
    payroll_share = (annualpayrollequivalent_demand)  # / annualpayrollequivalent_demand.sum()).round(2)
    payroll_share.plot(kind='pie', title='Share of Regional Payroll Demand', subplots=True)
    plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular
    plt.legend(loc='upper left', bbox_to_anchor=(-0.1, 1.))
    plt.show()

    # Plot 2 - Pie Chart for Payroll Demand
    annualpayrollequivalent_demand = new[['salary_predicted', 'cluster3and4']].groupby(['cluster3and4']).size()
    payroll_share = (annualpayrollequivalent_demand)  # / annualpayrollequivalent_demand.sum()).round(2)
    payroll_share.plot(kind='pie', title='Share of Regional Payroll Demand', subplots=True)
    plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular
    plt.legend(loc='upper left', bbox_to_anchor=(-0.1, 1.))
    plt.show()

    return new

def generate_eita(new, gdf):
    pp = new.copy()
    y = pp.groupby(['LAD22CD', 'cluster3and4']).size() / pp.groupby(['LAD22CD']).size()
    ydf = pd.concat([y, pp.groupby(['LAD22CD', 'cluster3and4'])[['salary_predicted']].mean()], axis=1)
    rydf = ydf.reset_index()
    eita = pd.merge(rydf, gdf, left_on='LAD22CD', right_on='reference')
    eita = eita[eita.LAD22CD.isin(lad_codes)]

    return eita

def plot_job_cluster_share(eita):
    eita = eita[eita.LAD22CD.isin(lad_codes)]

    for ii in eita.cluster3and4.unique():
        e = eita[eita['cluster3and4'] == ii]
        gdf1 = gpd.GeoDataFrame(e, geometry='geometry')
        fig, ax = plt.subplots(figsize=(10, 10))
        gdf1.plot(ax=ax, column=0, legend=True, cmap='RdYlBu', scheme='BoxPlot')
        ax.set_title('Job Cluster Share ' + str(ii))
        ax.set_xticks([])
        ax.set_yticks([])
        # Annotate LAD22CD on top of each center
        for idx, row in gdf1.iterrows():
            center = xs[xs.LAD22CD==row['LAD22CD']]['LAD22NM'].values[0]
            x, y = row['geometry'].centroid.coords[0]
            ax.text(x, y, center, fontsize=10, ha='center', va='center', color='black')

def plot_salary_predicted(eita):

    eita = eita[eita.LAD22CD.isin(lad_codes)]

    for ii in eita.cluster3and4.unique():
        e = eita[eita['cluster3and4'] == ii]
        gdf1 = gpd.GeoDataFrame(e, geometry='geometry')
        fig, ax = plt.subplots(figsize=(10, 10))
        gdf1.plot(ax=ax, column='salary_predicted', legend=True, cmap='RdYlBu', scheme='BoxPlot')
        ax.set_title('Salary Predicted by SOC Code ' + str(ii))
        ax.set_xticks([])
        ax.set_yticks([])
        
        # Annotate LAD22CD on top of each center
        for idx, row in gdf1.iterrows():
            center = xs[xs.LAD22CD==row['LAD22CD']]['LAD22NM'].values[0]
            x, y = row['geometry'].centroid.coords[0]
            ax.text(x, y, center, fontsize=10, ha='center', va='center', color='black')

def analyze_top_subregions(new):
    n = new[['LAD22CD', 'category_name', 'salary_predicted']]
    n.LAD22CD=n.LAD22CD.map(xs['LAD22NM'].to_dict())
    # Top subregions by category count
    zz_count = n.groupby(['category_name', 'LAD22CD']).size().sort_values().reset_index()
    for category in zz_count.category_name.unique():
        top_subregions = zz_count[zz_count['category_name'] == category].sort_values(by=0).tail()
        print(f"Top subregions for category {category} (by count):\n{top_subregions}\n")

    # Top subregions by average salary
    zz_salary = n.groupby(['category_name', 'LAD22CD']).mean().sort_values(by='salary_predicted').reset_index()
    for category in zz_salary.category_name.unique():
        top_subregions = zz_salary[zz_salary['category_name'] == category].sort_values(by='salary_predicted').tail()
        print(f"Top subregions for category {category} (by average salary):\n{top_subregions}\n")

import seaborn as sns
import matplotlib.pyplot as plt

def create_heatmap(data, x_column, y_column, values_column, aggfunc='mean', cmap='RdYlBu'):
    # Generate the pivot table
    heat = pd.pivot_table(data, columns=x_column, index=y_column, values=values_column, aggfunc=aggfunc)    
    heat.columns=heat.columns.map(xs['LAD22NM'].to_dict())
    # Reorder the columns and rows based on the sum of values
    cols = heat.sum().sort_values().index[::-1]
    idx = heat.sum(axis=1).sort_values().index[::-1]

    heat = heat.loc[idx[3:], cols].fillna(0)
    heat = heat.drop('Unknown')

    # Create the heat map using seaborn
    plt.figure(figsize=(10, 8))
    sns.heatmap(heat, cmap=cmap, fmt='.0f', linewidths=0.5, cbar=True)

    # Set the axis labels and title
    plt.xlabel(x_column)
    plt.ylabel(y_column)
    plt.title(f"Heat Map of {x_column} by {y_column}")

    # Rotate the tick labels for better visibility
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)

    # Display the heat map
    plt.show()

#@title
#stats and counts
newfull=visualize_data('full_time')
newpart=visualize_data('part_time')

analyze_top_subregions(newfull)
analyze_top_subregions(newpart)

create_heatmap(newfull, 'LAD22CD', 'category_name', 'salary_predicted', aggfunc='mean', cmap='RdYlBu')
create_heatmap(newpart, 'LAD22CD', 'category_name', 'salary_predicted', aggfunc='mean', cmap='RdYlBu')

#@title
def plot_salary_predicted(eita):


    eita = eita[eita.LAD22CD.isin(lad_codes)]

    for ii in eita.cluster3and4.unique():
        e = eita[eita['cluster3and4'] == ii]
        gdf1 = gpd.GeoDataFrame(e, geometry='geometry')
        fig, ax = plt.subplots(figsize=(10, 10))
        gdf1.plot(ax=ax, column='salary_predicted', legend=True, cmap='RdYlBu', scheme='BoxPlot')
        ax.set_title('Salary Predicted by SOC Code ' + str(ii))
        ax.set_xticks([])
        ax.set_yticks([])
        
        # Annotate LAD22CD on top of each center
        for idx, row in gdf1.iterrows():
            center = xs[xs.LAD22CD==row['LAD22CD']]['LAD22NM'].values[0]
            x, y = row['geometry'].centroid.coords[0]
            ax.text(x, y, center, fontsize=10, ha='center', va='center', color='black')
        
        plt.show()
plot_salary_predicted(generate_eita(newfull, gdf))

#maps
plot_job_cluster_share(generate_eita(newfull, gdf))
plot_salary_predicted(generate_eita(newfull, gdf))
plot_job_cluster_share(generate_eita(newpart, gdf))
plot_salary_predicted(generate_eita(newpart, gdf))

"""###warrington - filter wards - skills mentioned - time series - heatmap ward - **piechart**"""

#@title
import pandas as pd
names=['Cheshire and Warrington','Warrington']
paths=('/content/drive/MyDrive/AUTONOMY/20230508CheshireAndWarringtonAdzunaBaseline.csv'),('/content/drive/MyDrive/AUTONOMY/20230427WarringtonAdzunaBaseline.csv') #('/content/drive/MyDrive/AUTONOMY/20230508NorthWestAdzunaBaseline.csv'),
r=pd.DataFrame(names,paths).reset_index()
df=pd.read_csv(r[r[0]=='Warrington']['index'].values[0])
k=df.copy()

#@title
k=k[['filename','LAD22CD', 'salary_predicted', 'contract_time', 'soc2020', 'soc2020_minor_group', 'id', 'category_name','soc2020_major_group','wd19cd','wards']]

#@title
import pandas as pd
from datetime import datetime
kob2 = []
b=[]
for file_path in k['filename'].values:
    if 'day=' not in file_path:
        continue
    month = int(file_path.split('month=')[1].split('/')[0])
    #day = int(file_path.split('day=')[1].split('/')[0])
    #print(f"Month: {month}")
    #print(f"Day: {day}")
    kob2.append(datetime(2022, month,28))
    b.append(month)

k['date']=pd.Series(kob2)
k['m']=b
#create monthly tables for full time and part time soc1 per ward per month
import pandas as pd
def create_pivot_table(df, values_col, index_col, column_col):
    pivot_table = pd.pivot_table(df, values=values_col, index=[index_col], columns=[column_col], aggfunc='sum')
    return pivot_table
#fulltime tables
_time='full_time'
c=[]
for i in k.m.unique():
  df=k.copy()
  df=df[df['contract_time']==_time]
  df=df[df['m']==i]
  pdf=df[['wards','soc2020_major_group']].groupby(['wards','soc2020_major_group']).size().reset_index()
  c.append(create_pivot_table(pdf, 0, 'wards', 'soc2020_major_group'))

import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis
    pivot_table.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))
    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')
# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')
# adjust layout to prevent overlap of axis labels
fig.tight_layout()

import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis

    # assuming your dataframe is called `df` and the indexes are in a column called `index_col`
    # create a dictionary to map the indexes to the groups
    groups = { 'Bewsey and Whitecross': 'Bewsey and Whitecross',
              **{index: 'All the Rest' for index in pivot_table.index if index not in [ 'Bewsey and Whitecross']}}


    # add a new column to the dataframe with the group labels
    pivot_table['group'] = [groups[index] for index in pivot_table.index]

    # group the dataframe by the group column and sum the values
    pivot_table = pivot_table.groupby('group').sum()

    pivot_table.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))

    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')
# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')
# adjust layout to prevent overlap of axis labels
fig.tight_layout()


#parttime tables
_time='part_time'
c=[]
for i in k.m.unique():
  df=k.copy()
  df=df[df['contract_time']==_time]
  df=df[df['m']==i]
  pdf=df[['wards','soc2020_major_group']].groupby(['wards','soc2020_major_group']).size().reset_index()
  c.append(create_pivot_table(pdf, 0, 'wards', 'soc2020_major_group'))

import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis
    pivot_table.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))
    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')
# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')
# adjust layout to prevent overlap of axis labels
fig.tight_layout()

import matplotlib.pyplot as plt

fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 9), sharex=True, sharey=True)

for i, pivot_table in enumerate(c):
    row = i // 3
    col = i % 3
    ax = axs[row, col]
    # plot pivot table on current axis

    # assuming your dataframe is called `df` and the indexes are in a column called `index_col`
    # create a dictionary to map the indexes to the groups
    groups = {'Appleton': 'Appleton', 'Bewsey and Whitecross': 'Bewsey and Whitecross',
              **{index: 'All the Rest' for index in pivot_table.index if index not in ['Appleton', 'Bewsey and Whitecross']}}


    # add a new column to the dataframe with the group labels
    pivot_table['group'] = [groups[index] for index in pivot_table.index]

    # group the dataframe by the group column and sum the values
    pivot_table = pivot_table.groupby('group').sum()

    pivot_table.plot(ax=ax, legend=False, kind='bar', stacked=True, label='Month {}'.format(i+1))

    # set title for current axis
    ax.set_title('Month {}'.format(i+1))
    
# add common x-axis label
fig.text(0.5, 0.04, 'Date', ha='center', va='center')
# add common y-axis label
fig.text(0.06, 0.5, 'Number of Contracts', ha='center', va='center', rotation='vertical')
# create legend
handles, labels = axs[0, 0].get_legend_handles_labels()
fig.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')
# adjust layout to prevent overlap of axis labels
fig.tight_layout()

#@title
!unzip '/content/geodb_DBO_WD_DEC_2019_GB_BFE_2022_-6013351099376194804.zip'
gdf = gpd.read_file('/content/geodb_DBO_WD_DEC_2019_GB_BFE.shp', crs="EPSG:4326")
z=gdf[gdf['wd19nm'].isin(['Appleton','Bewsey and Whitecross','Birchwood','Burtonwood and Winwick','Chapelford and Old Hall','Culcheth, Glazebury and Croft','Fairfield and Howley','Grappenhall','Great Sankey North and Whittle Hall','Great Sankey South','Latchford East','Latchford West','Lymm North and Thelwall','Lymm South','Orford','Penketh and Cuerdley','Poplars and Hulme','Poulton North','Poulton South','Rixton and Woolston','Stockton Heath','Westbrook'])]
wards=z.loc[(z['lat'] > -2.67) & (z['long'] < -2.47)]
k=k.merge(wards.iloc[1:])[['salary_predicted', 'contract_time', 'soc2020',
       'soc2020_minor_group', 'id', 'category_name', 'soc2020_major_group',
       'wd19cd', 'wards', 'date', 'm', 'wd19nm', 'geometry']]

#@title
def visualize_data(_time):
    # Step 1
    df2 = k.copy()
    df2 = df2[df2['contract_time'] == _time]
    kdf = df2.drop_duplicates('id')

    # Step 2
    sonia4clusters = pd.read_csv('/content/drive/MyDrive/AUTONOMY/20230512AspectClustering.csv')
    d = sonia4clusters[['SOC', 'soniasexpandedfinalfix']]
    d.index = d.SOC
    d_dict = d['soniasexpandedfinalfix'].to_dict()

    # Step 3
    kdf['cluster3'] = kdf['soc2020_minor_group'].map(d_dict)
    kdf['cluster4'] = kdf['soc2020'].map(d_dict)
    new = kdf.loc[kdf['cluster3'].dropna().index]
    new['cluster3and4'] = np.where(pd.isna(new['cluster4']), new['cluster3'], new['cluster4'])

    # Step 5
    kdf['soc2020'].dropna().hist()
    plt.show()

    # Step 6
    kdf['soc2020_minor_group'].dropna().hist()
    plt.show()

    # Step 5
    news = new.groupby(['cluster3and4', 'm']).size()
    pivot_table = pd.pivot_table(news.reset_index(), columns='cluster3and4', index='m', values=0)
    pivot_table=pivot_table.sort_index()
    pivot_table.plot(kind='bar', stacked=True)

    # Step 5
    pv = pd.pivot_table(new, columns='cluster3and4', index='m', values='salary_predicted', aggfunc='mean')
    pv.plot()

    # Plot 1 - Pie Chart for Payroll Demand
    annualpayrollequivalent_demand = new[['salary_predicted', 'cluster3and4']].groupby(['cluster3and4']).sum()
    payroll_share = (annualpayrollequivalent_demand)  # / annualpayrollequivalent_demand.sum()).round(2)
    payroll_share.plot(kind='pie', title='Share of Regional Payroll Demand', subplots=True)
    plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular
    plt.legend(loc='upper left', bbox_to_anchor=(-0.1, 1.))
    plt.show()

    # Plot 2 - Pie Chart for Payroll Demand
    annualpayrollequivalent_demand = new[['salary_predicted', 'cluster3and4']].groupby(['cluster3and4']).size()
    payroll_share = (annualpayrollequivalent_demand)  # / annualpayrollequivalent_demand.sum()).round(2)
    payroll_share.plot(kind='pie', title='Share of Regional Payroll Demand', subplots=True)
    plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular
    plt.legend(loc='upper left', bbox_to_anchor=(-0.1, 1.))
    plt.show()
    
    return new

def generate_eita(new):
    pp = new.copy()
    y = pp.groupby(['wards', 'cluster3and4']).size() / pp.groupby(['wards']).size()
    ydf = pd.concat([y, pp.groupby(['wards', 'cluster3and4'])[['salary_predicted']].mean()], axis=1)
    rydf = ydf.reset_index()
    rydf=rydf.merge(wards.iloc[1:],left_on='wards',right_on='wd19nm')[['wards', 'cluster3and4', 0, 'salary_predicted','geometry']]
    return rydf
eita=generate_eita(newfull)
def plot_job_cluster_share(eita):
    for ii in eita.cluster3and4.unique():
        e = eita[eita['cluster3and4'] == ii]
        gdf1 = gpd.GeoDataFrame(e, geometry='geometry')
        fig, ax = plt.subplots(figsize=(10, 10))
        gdf1.plot(ax=ax, column=0, legend=True, cmap='RdYlBu', scheme='BoxPlot')
        ax.set_title('Job Cluster Share ' + str(ii))
        ax.set_xticks([])
        ax.set_yticks([])
        # Annotate LAD22CD on top of each center
        for idx, row in gdf1.iterrows():
            center = gdf1[gdf1.wards==row['wards']]['wards'].values[0]
            x, y = row['geometry'].centroid.coords[0]
            ax.text(x, y, center, fontsize=10, ha='center', va='center', color='black')

def plot_salary_predicted(eita):

    for ii in eita.cluster3and4.unique():
        e = eita[eita['cluster3and4'] == ii]
        gdf1 = gpd.GeoDataFrame(e, geometry='geometry')
        fig, ax = plt.subplots(figsize=(10, 10))
        gdf1.plot(ax=ax, column='salary_predicted', legend=True, cmap='RdYlBu', scheme='BoxPlot')
        ax.set_title('Job Cluster salary_predicted per ward ' + str(ii))
        ax.set_xticks([])
        ax.set_yticks([])
        # Annotate LAD22CD on top of each center
        for idx, row in gdf1.iterrows():
            center = gdf1[gdf1.wards==row['wards']]['wards'].values[0]
            x, y = row['geometry'].centroid.coords[0]
            ax.text(x, y, center, fontsize=10, ha='center', va='center', color='black')

#@title
_time='full_time'
newfull=visualize_data(_time)
print('missing wards fulltime',set(['Appleton','Bewsey and Whitecross','Birchwood','Burtonwood and Winwick','Chapelford and Old Hall','Culcheth, Glazebury and Croft','Fairfield and Howley','Grappenhall','Great Sankey North and Whittle Hall','Great Sankey South','Latchford East','Latchford West','Lymm North and Thelwall','Lymm South','Orford','Penketh and Cuerdley','Poplars and Hulme','Poulton North','Poulton South','Rixton and Woolston','Stockton Heath','Westbrook'])-set(newfull.wards.unique()))
_time='part_time'
newpart=visualize_data(_time)
print('missing wards parttime',set(['Appleton','Bewsey and Whitecross','Birchwood','Burtonwood and Winwick','Chapelford and Old Hall','Culcheth, Glazebury and Croft','Fairfield and Howley','Grappenhall','Great Sankey North and Whittle Hall','Great Sankey South','Latchford East','Latchford West','Lymm North and Thelwall','Lymm South','Orford','Penketh and Cuerdley','Poplars and Hulme','Poulton North','Poulton South','Rixton and Woolston','Stockton Heath','Westbrook'])-set(newpart.wards.unique()))
plot_job_cluster_share(generate_eita(newfull))
plot_salary_predicted(generate_eita(newfull))
plot_job_cluster_share(generate_eita(newpart))
plot_salary_predicted(generate_eita(newpart))

"""Stemword of Demanded Skills in Warrington crossreferenced through using NLP (chatgpt)"""

#@title
import pandas as pd
from nltk.stem import PorterStemmer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import ast
import re

ppppp=pd.read_csv('/content/drive/MyDrive/temp2/skills_final_20230514232800.csv')
kp=k.merge(ppppp)
kp=kp.drop_duplicates('id')
ftpt='full_time'
kp=kp[kp['contract_time']==ftpt]
kp.groupby(['category_name']).size().sort_values().plot(kind='pie',title='Total Unique '+str(len(kp))+' '+ftpt+' jobs')
for c in kp.category_name.unique():
  kpsub=kp[kp['category_name']==c]
  ai=[]
  for i in kpsub['skills']:
    data_str = i
    data_list = ast.literal_eval(data_str)
    data_list_filtered = [item for item in data_list if item != '']
    ai.append(pd.Series(data_list_filtered))
  z=pd.concat(ai)
  # Group the data based on the values
  groups = z.groupby(z)
  # Get the counts of each group
  group_counts = groups.size()

  # Convert index to string
  group_counts.index = group_counts.index.astype('str')

  series = group_counts.copy()
  # Filter words with more than 15 characters
  filtered_series = series[series.index.astype('str').str.len() > 12]

  data = filtered_series.copy()
  # Initialize the PorterStemmer
  stemmer = PorterStemmer()

  # Generate word stems for the index
  stems = [stemmer.stem(re.sub(r'[^a-zA-Z]', '', word)) for word in data.index]

  # Create a new Series with stems as the index
  stemmed_series = pd.Series(data.values, index=stems)

  # Get the top 50 words with highest values
  top_words = stemmed_series.groupby(stemmed_series.index).sum().sort_values().tail(50).index

  # Retrieve the original full words corresponding to the top stems
  original_words = [word for word in data.index if stemmer.stem(re.sub(r'[^a-zA-Z]', '', word)) in top_words]

  # Create a word cloud from the original full words
  wordcloud = WordCloud().generate_from_frequencies(dict(data[original_words]))

  # Display the word cloud
  plt.figure(figsize=(10, 6))
  plt.title('Most Common Skill Requirements for '+ ftpt+' in '+ c)
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')
  plt.show()

ppppp=pd.read_csv('/content/drive/MyDrive/temp2/skills_final_20230514232800.csv')
kp=k.merge(ppppp)
kp=kp.drop_duplicates('id')
ftpt='contract time not specified'
#kp=kp[kp['contract_time']==ftpt]
kp.groupby(['category_name']).size().sort_values().plot(kind='pie',title='Total Unique '+str(len(kp))+' '+ftpt+' jobs')
for c in kp.category_name.unique():
  kpsub=kp[kp['category_name']==c]
  ai=[]
  for i in kpsub['skills']:
    data_str = i
    data_list = ast.literal_eval(data_str)
    data_list_filtered = [item for item in data_list if item != '']
    ai.append(pd.Series(data_list_filtered))
  z=pd.concat(ai)
  # Group the data based on the values
  groups = z.groupby(z)
  # Get the counts of each group
  group_counts = groups.size()

  # Convert index to string
  group_counts.index = group_counts.index.astype('str')

  series = group_counts.copy()
  # Filter words with more than 15 characters
  filtered_series = series[series.index.astype('str').str.len() > 12]

  data = filtered_series.copy()
  # Initialize the PorterStemmer
  stemmer = PorterStemmer()

  # Generate word stems for the index
  stems = [stemmer.stem(re.sub(r'[^a-zA-Z]', '', word)) for word in data.index]

  # Create a new Series with stems as the index
  stemmed_series = pd.Series(data.values, index=stems)

  # Get the top 50 words with highest values
  top_words = stemmed_series.groupby(stemmed_series.index).sum().sort_values().tail(50).index

  # Retrieve the original full words corresponding to the top stems
  original_words = [word for word in data.index if stemmer.stem(re.sub(r'[^a-zA-Z]', '', word)) in top_words]

  # Create a word cloud from the original full words
  wordcloud = WordCloud().generate_from_frequencies(dict(data[original_words]))

  # Display the word cloud
  plt.figure(figsize=(10, 6))
  plt.title('Most Requirements for '+ ftpt+' in '+ c)
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')
  plt.show()

#@title
ppppp=pd.read_csv('/content/drive/MyDrive/temp2/skills_final_20230514232800.csv')
kp=k.merge(ppppp)
kp=kp.drop_duplicates('id')
ftpt='full_time'
kp=kp[kp['contract_time']==ftpt]
kp.groupby(['wards']).size().sort_values().plot(kind='pie',title='Total Unique '+str(len(kp))+' '+ftpt+' jobs')
for c in kp['wards'].unique():
  kpsub=kp[kp['wards']==c]
  ai=[]
  for i in kpsub['skills']:
    data_str = i
    data_list = ast.literal_eval(data_str)
    data_list_filtered = [item for item in data_list if item != '']
    ai.append(pd.Series(data_list_filtered))
  z=pd.concat(ai)
  # Group the data based on the values
  groups = z.groupby(z)
  # Get the counts of each group
  group_counts = groups.size()

  # Convert index to string
  group_counts.index = group_counts.index.astype('str')

  series = group_counts.copy()
  # Filter words with more than 15 characters
  filtered_series = series[series.index.astype('str').str.len() > 12]

  data = filtered_series.copy()
  # Initialize the PorterStemmer
  stemmer = PorterStemmer()

  # Generate word stems for the index
  stems = [stemmer.stem(re.sub(r'[^a-zA-Z]', '', word)) for word in data.index]

  # Create a new Series with stems as the index
  stemmed_series = pd.Series(data.values, index=stems)

  # Get the top 50 words with highest values
  top_words = stemmed_series.groupby(stemmed_series.index).sum().sort_values().tail(50).index

  # Retrieve the original full words corresponding to the top stems
  original_words = [word for word in data.index if stemmer.stem(re.sub(r'[^a-zA-Z]', '', word)) in top_words]

  # Create a word cloud from the original full words
  wordcloud = WordCloud().generate_from_frequencies(dict(data[original_words]))

  # Display the word cloud
  plt.figure(figsize=(10, 6))
  plt.title('Most Common Requirements for '+ ftpt+' in '+ c)
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')
  plt.show()

"""##Demand Forecast Warwick Cheshire and Warrington

adzuna cheshire and warrington baseline[link text](https://)
"""

#@title
!wget 'https://files.planning.data.gov.uk/dataset/local-authority-district.geojson'
!pip install geopandas
import pandas as pd
!pip install geopandas
!pip install contextily
!pip install descartes
!pip install shapely
!pip install mapclassify
import geopandas as gpd
import matplotlib.pyplot as plt
import descartes
import shapely
import seaborn as sns
from shapely.geometry import Point, Polygon
import mapclassify
import seaborn as sns
import matplotlib.cm as cm
import geopandas as gpd
# Read the GeoJSON file using GeoPandas
gdf = gpd.read_file('/content/local-authority-district.geojson')
# Print the GeoDataFrame
print(gdf)

#@title
import pandas as pd
import geopandas as gpd

names=['Cheshire and Warrington','Warrington']
paths=('/content/drive/MyDrive/AUTONOMY/20230508CheshireAndWarringtonAdzunaBaseline.csv'),('/content/drive/MyDrive/AUTONOMY/20230427WarringtonAdzunaBaseline.csv') #('/content/drive/MyDrive/AUTONOMY/20230508NorthWestAdzunaBaseline.csv'),
r=pd.DataFrame(names,paths).reset_index()
df=pd.read_csv(r[r[0]=='Cheshire and Warrington']['index'].values[0])
k=df.copy()
k=k[['filename','LAD22CD', 'salary_predicted', 'contract_time', 'soc2020', 'soc2020_minor_group', 'id', 'category_name','soc2020_major_group']]
k=k.drop_duplicates('id')
gdf = gpd.read_file('/content/local-authority-district.geojson')
CandW=k[k['LAD22CD'].isin(gdf[gdf.name.isin(['Cheshire East',  'Cheshire West and Chester','Warrington'])]['reference'].unique())]
CandWPT=CandW[CandW['contract_time']=='part_time']
CandWFT=CandW[CandW['contract_time']=='full_time']
print(CandWPT.groupby(['soc2020_major_group']).size())
print(CandWFT.groupby(['soc2020_major_group']).size())

"""warwick forecasts for cheshire adn warrington

have to figure out a way to plug ons shares of warrington in warrington + cheshire set
"""

import pandas as pd
names=['Cheshire and Warrington','Warrington']
paths=('/content/drive/MyDrive/AUTONOMY/20230508CheshireAndWarringtonAdzunaBaseline.csv'),('/content/drive/MyDrive/AUTONOMY/20230427WarringtonAdzunaBaseline.csv') #('/content/drive/MyDrive/AUTONOMY/20230508NorthWestAdzunaBaseline.csv'),
r=pd.DataFrame(names,paths).reset_index()
df=pd.read_csv(r[r[0]=='Cheshire and Warrington']['index'].values[0])
k=df.copy()
p=pd.pivot_table(k,columns='soc2020_submajor_group',index='LAD22CD', aggfunc='count')[['Unnamed: 0']]
warringtonshares=(p.loc['E06000007']/p.sum())
warringtonshares=warringtonshares.round(2)
warringtonshares.index=warringtonshares.index.get_level_values(1)
print('as demand',warringtonshares)

numberofemployed=pd.read_excel('/content/drive/MyDrive/Autonomy Data Unit v2/Projects/warrington_project/Datasets/SOC_by_LA_LEP.xlsx',sheet_name=1) #APS
numberofemployed=numberofemployed[numberofemployed['Unnamed: 1']=='E37000003  Cheshire and Warrington']
numberofemployed.columns=pd.read_excel('/content/drive/MyDrive/Autonomy Data Unit v2/Projects/warrington_project/Datasets/SOC_by_LA_LEP.xlsx',sheet_name=1).loc[6]
numberofemployed=numberofemployed.T
numberofemployed.index=numberofemployed.index.str[:2]
numberofemployed=numberofemployed.iloc[3:]
numberofemployed=numberofemployed.replace('*','')
numberofemployed[9] = pd.to_numeric(numberofemployed[9], errors='coerce')
candw=numberofemployed.reset_index().groupby(6).sum()

numberofemployed=pd.read_excel('/content/drive/MyDrive/Autonomy Data Unit v2/Projects/warrington_project/Datasets/SOC_by_LA_LEP.xlsx',sheet_name=3) #APS
numberofemployed=numberofemployed[numberofemployed['Unnamed: 1']=='E06000007 Warrington']
numberofemployed.columns=pd.read_excel('/content/drive/MyDrive/Autonomy Data Unit v2/Projects/warrington_project/Datasets/SOC_by_LA_LEP.xlsx',sheet_name=3).loc[6]
numberofemployed=numberofemployed.T
numberofemployed.index=numberofemployed.index.str[:2]
numberofemployed=numberofemployed.iloc[3:]
numberofemployed=numberofemployed.replace('*','')
numberofemployed[9] = pd.to_numeric(numberofemployed[14], errors='coerce')
w=numberofemployed.reset_index().groupby(6).sum()
print('as supply?',w/candw)
wratio=w/candw
wratio.index.name=''
wratio[9]
#@title
!pip install odfpy
import os
import odf
from odf import text, teletype, table

# Specify the file path
file_path = '/content/Cheshire_and_Warrington_LEP_Main_Tables.ods'

# Check if the file already exists
if not os.path.exists(file_path):
    # Download the file if it doesn't exist
    !unzip '/content/drive/MyDrive/Autonomy Data Unit v2/Projects/warrington_project/Labour_market_and_skills_projections_-_LEPs.zip'

import pandas as pd
# Replace 'file.ods' with the path to your ODS file
dataframes = pd.read_excel('/content/LEPs/Cheshire_and_Warrington_LEP_Main_Tables.ods', sheet_name=None)

for sheet_name, df in dataframes.items():
    # Rename the dataframe object with the sheet name
    globals()[sheet_name] = df

# Get the names of all dataframe objects created
dataframe_names = [name for name, obj in globals().items() if isinstance(obj, pd.core.frame.DataFrame)]

o=Occ_F2[Occ_F2.loc[2][15:].index].dropna()
o.columns=Occ_F2.loc[2][15:].values

# Assuming 'Occ_F2' is your DataFrame
# Create a boolean mask for rows starting with two-digit numbers, excluding missing values
mask = Occ_F2['Unnamed: 11'].str.match(r'^\d{2}')
mask = mask & ~Occ_F2['Unnamed: 11'].isna()

# Select rows based on the mask using .loc
selected_rows = o.loc[mask]

selected_rows.index=Occ_F2['Unnamed: 11'].loc[mask].values

# Print the selected rows
data = selected_rows.T
ax = data.plot(kind='bar', stacked=True, title='Cheshire & Warrington WorkForce SOC 3 Digits Projection', ylabel='Thdsd', rot=0)
plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')

plt.show()


#per gender projections
ALL=Occ_T2.iloc[:,:11]
MALE=Occ_T2.iloc[:,11:22]
FEMALE=Occ_T2.iloc[:,22:]

df = ALL.dropna(axis=1, how='all').dropna(axis=0, how='all').copy()

listarows=df[df.iloc[:,0].isin(['Levels','Shares','Growth', 'Change'])].index
names=['Levels (000s)','Shares (per cent)','Growth', 'Change']
import numpy as np
k = {}
for i in np.arange(0,len(listarows)):
  start_row=listarows[i-1]
  try:
    end_row=listarows[i+1]
  except:
    end_row=None
  
  k[(names[i]+'All')] = df.iloc[start_row:end_row].dropna(axis=0, how='all').to_dict()
#pd.DataFrame(k['Managers, directors and senior officialsfem'])#
k.keys()

df = MALE.dropna(axis=1, how='all').dropna(axis=0, how='all').copy()

listarows=df[df.iloc[:,0].isin(['Levels','Shares','Growth', 'Change'])].index
names=['Levels (000s)','Shares (per cent)','Growth', 'Change']

for i in np.arange(0,len(listarows)):
  start_row=listarows[i-1]
  try:
    end_row=listarows[i+1]
  except:
    end_row=None
  
  k[(names[i]+'Male')] = df.iloc[start_row:end_row].dropna(axis=0, how='all').to_dict()
#pd.DataFrame(k['Managers, directors and senior officialsfem'])#
k.keys()


# Create a copy of the dataframe
df = FEMALE.dropna(axis=1, how='all').dropna(axis=0, how='all').copy()

listarows=df[df.iloc[:,0].isin(['Levels','Shares','Growth', 'Change'])].index
names=['Levels (000s)','Shares (per cent)','Growth', 'Change']

for i in np.arange(0,len(listarows)):
  start_row=listarows[i]

  try:
    end_row=listarows[i+1]
  except:
    end_row=None
  
  k[(names[i]+'fem')] = df.loc[start_row:end_row].to_dict()
#pd.DataFrame(k['Managers, directors and senior officialsfem'])#
k.keys()

ALL=Occ_T2.iloc[:,:11]
ALL.loc[ALL.iloc[:,0].dropna().index]

# Assuming 'Occ_F2' is your DataFrame
# Create a boolean mask for rows starting with two-digit numbers, excluding missing values
mask = ALL.iloc[:,0].str.match(r'^\d{2}')
mask = mask & ~ ALL.iloc[:,0].isna()
A=ALL[mask].iloc[1:27]
A.index=A.iloc[:,0].astype('str').str[:2]
A=A.iloc[:,1:6]
A.columns=['2015','2020','2025','2030','2035']
A.index.name=''
wratio.index.name=''
set(A.index)-set(wratio.index)
import matplotlib.pyplot as plt

data = (A.T * wratio[9]).T.T
ax = data[data.sum().sort_values(ascending=False).index].plot(kind='bar', stacked=True, title='Warrington WorkForce SOC 2 Digits Projection', ylabel='Thsds', rot=0)
plt.legend(ncol=6, loc='lower center', bbox_to_anchor=(0.5, -0.475))

plt.show()


import matplotlib.pyplot as plt

data = (A.T).T.T
ax = data[data.sum().sort_values(ascending=False).index].plot(kind='bar', stacked=True, title='Cheshire & Warrington WorkForce SOC 2 Digits Projection', ylabel='Thdsd', rot=0)
plt.legend(ncol=6, loc='lower center', bbox_to_anchor=(0.5, -0.475))

plt.show()

"""##Supply (ONS data on filled jobs and the skills used)

"""

#recover code

"""#Part3 - Survey

Acknowledge that we’ve talked to all the right people

"""

#@title
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import math

# Read in the Excel file
x = pd.read_excel('/content/Warrington skills commission data (1).xlsx')
ss=[]
# Loop through every column in the dataset
for col in x.columns:
    if x[col].dtype == 'O':  # Only perform the operation on object (string) columns
        # Count the number of responses in each category
        grouped = x[col].value_counts().reset_index(name='count')

        # Sort the data by count in descending order
       
        sorted_data = grouped.sort_values('count', ascending=False)
        sorted_data=sorted_data[sorted_data['index']!='-']
        ss.append(sorted_data)

        # Create a new figure for the plot
        plt.figure()

        # Create a horizontal bar chart using seaborn
        sns.set(style="whitegrid")
        chart = sns.barplot(x='count', y='index', data=sorted_data, color='skyblue')

        # Add labels to the chart
        middle_index = math.ceil(len(col) / 2)
        last_space_index = col[:middle_index].rfind(' ')
        broken_phrase = col[:last_space_index] + '\n' + col[last_space_index+1:]
        chart.set(xlabel='Number of Responses', ylabel=broken_phrase)
        chart.set_title(f"Survey Results for {col}")

        # Show the chart
        plt.show()

#@title
import seaborn as sns
import matplotlib.pyplot as plt

# Create a dictionary to store the Series based on their indexes
#series_dict = {}
g=[]
shit=[]
# Loop through the list of Series
for s in ss[2:]:
    # Rename the index column temporarily to avoid conflicts
    try:
      g.append(str(s['index'].sort_values().values))
    except:
      shit.append(s)
sss=pd.Series(g).reset_index()
sss.columns=['locoriginal','uniques']
sss.locoriginal=sss.locoriginal.astype('str')+','
b=sss.groupby('uniques').agg({'locoriginal': 'sum', 'uniques': 'count'}).uniques>1
bb=sss.groupby('uniques').agg({'locoriginal': 'sum', 'uniques': 'count'})
bb.columns=['locoriginal','counts']
bb[b].sort_values(by='counts')


data=bb[b].sort_values(by='counts')['locoriginal'].to_dict()

for key, value in data.items():
    value_list = [int(x) for x in value.split(',') if x.strip()]
    data[key] = value_list

kob=[]
for kk in data.values():
  a=[]
  for ki in kk:    
    z=ss[ki+2]
    name=x.columns[ki+2]
    z.index=z['index']
    z=z['count']
    middle_index = math.ceil(len(name) / 2)
    last_space_index = name[:middle_index].rfind(' ')
    broken_phrase = name[:last_space_index] + '\n' + name[last_space_index+1:]
    z.name=broken_phrase  
    a.append(z)  
  kob.append(pd.concat(a,axis=1))

import matplotlib.pyplot as plt
import numpy as np

for i, df in enumerate(kob):
    num_bars = df.shape[1] - 1  # Exclude the index column
    max_label_length = max([len(label) for label in df.columns[:-1]])
    figure_height = 0.15 * (num_bars + 2) + 0.035 * max_label_length
    if num_bars > 10:
        plt.rcParams["figure.figsize"] = (10, figure_height + num_bars * 0.25)
    else:
        plt.rcParams["figure.figsize"] = (10, figure_height)

    plt.figure()
    df_sorted = df.loc[df.T.iloc[-1].sort_values().index, df.iloc[-1].sort_values().index]
    ax = df_sorted.T.plot(kind='barh', stacked=True)
    plt.title(f'Plot {i+1}')
    plt.xlabel('X-axis label')
    plt.ylabel('Y-axis label')

    # Adjust spacing between y-labels
    num_ylabels = len(df_sorted.columns)
    yticks_positions = np.arange(num_ylabels)
    plt.yticks(yticks_positions, df_sorted.columns)
    plt.subplots_adjust(hspace=0.2)  # Adjust vertical spacing between subplots

    plt.show(f'plot_{i+1}.png')
    plt.close()



#@title
import seaborn as sns
import matplotlib.pyplot as plt

data = {}

# Loop through the list of Series
for s in ss[2:]:
    try:
        g.append(str(s['index'].sort_values().values))
    except:
        shit.append(s)

sss = pd.Series(g).reset_index()
sss.columns = ['locoriginal', 'uniques']
sss.locoriginal = sss.locoriginal.astype('str') + ','
b = sss.groupby('uniques').agg({'locoriginal': 'sum', 'uniques': 'count'}).uniques > 1
bb = sss.groupby('uniques').agg({'locoriginal': 'sum', 'uniques': 'count'})
bb.columns = ['locoriginal', 'counts']
bb = bb[b].sort_values(by='counts')

data = bb['locoriginal'].to_dict()

for key, value in data.items():
    value_list = [int(x) for x in value.split(',') if x.strip()]
    data[key] = value_list

kob = []
for kk in data.values():
    a = []
    for ki in kk:
        try:
            z = ss[ki+2]
            name = x.columns[ki+2]
            z.index = z['index']
            z = z['count']
            middle_index = math.ceil(len(name) / 2)
            last_space_index = name[:middle_index].rfind(' ')
            broken_phrase = name[:last_space_index] + '\n' + name[last_space_index+1:]
            z.name = broken_phrase  
            a.append(z)
        except IndexError:
            print(f"Index out of range for ki={ki}")
    kob.append(pd.concat(a, axis=1))

for df in kob:
    plt.figure()
    df = df.loc[df.T.iloc[-1].sort_values().index, df.iloc[-1].sort_values().index]
    df.plot(kind='barh', stacked=True, cmap='coolwarm')
    plt.show()

"""#Part4 Futures to prepare for / Economic Forecasts

Luiz

#High net hydrogen pipeline
(insert retrofit analysis for warrington)
"""